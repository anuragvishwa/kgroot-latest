# AI SRE Platform - Multi-Agent Root Cause Analysis System

> **Production-ready, Kafka-first, multi-agent AI SRE platform for automated incident investigation**

Built on top of KGroot causality analysis with lightweight, modular agent architecture.

---

## ðŸŽ¯ Vision

Build a **true AI SRE** that acts as an "agentic interface to production" by:

- Understanding infrastructure topology and codebase
- Integrating with observability tools (Datadog, Grafana, Argo, etc.)
- Capturing institutional knowledge from Slack, docs, and runbooks
- Reasoning across production environments to deliver high-confidence RCA
- Learning from past incidents to improve over time

**Not just ChatGPT + logs. Real autonomous investigation.**

---

## ðŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INGESTION LAYER (Kafka-First)                     â”‚
â”‚                                                                           â”‚
â”‚   Sources â†’ Kafka Topics (Multi-tenant) â†’ Stream Processing â†’ Storage    â”‚
â”‚                                                                           â”‚
â”‚   â€¢ K8s Events          â€¢ Datadog Metrics        â€¢ Argo Logs            â”‚
â”‚   â€¢ GitHub CI/CD        â€¢ Slack Incidents        â€¢ Prometheus           â”‚
â”‚                                                                           â”‚
â”‚   All data flows through Kafka with canonical event envelope            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PLANE (Always Available)                         â”‚
â”‚                                                                           â”‚
â”‚   â€¢ Neo4j (Causality + Topology + Vectors)                              â”‚
â”‚   â€¢ OpenSearch (Logs + Metrics + Full-text)                             â”‚
â”‚   â€¢ Redis (Session cache + Rate limiting)                               â”‚
â”‚   â€¢ PostgreSQL (Serviceâ†’Repo mapping)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTROL PLANE (AI Orchestration)                      â”‚
â”‚                                                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚            AI SRE Orchestrator (FastAPI)                    â”‚        â”‚
â”‚   â”‚  â€¢ Rule-based routing (cheap, deterministic)               â”‚        â”‚
â”‚   â”‚  â€¢ LLM planning (only for ambiguous cases)                 â”‚        â”‚
â”‚   â”‚  â€¢ Agent coordination & memory management                  â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚Tool Registry â”‚  â”‚Agent Registryâ”‚  â”‚Memory Managerâ”‚                 â”‚
â”‚   â”‚(Plugin-based)â”‚  â”‚(Dynamic Load)â”‚  â”‚(Context/LTM) â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SPECIALIST AGENTS (Modular)                          â”‚
â”‚                                                                           â”‚
â”‚   GraphAgent     MetricsAgent    LogsAgent     CodeAgent   ContextAgent â”‚
â”‚   (Neo4j RCA)    (Datadog/Prom)  (Argo/ES)     (GitHub)    (Slack/Docs)â”‚
â”‚                                                                           â”‚
â”‚   Each agent has access to specific tools from Tool Registry            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ—ï¸ Key Design Principles

### 1. **Kafka-First Ingestion**
- All data sources â†’ Kafka topics (with tenant isolation)
- Canonical event envelope for consistency
- Exactly-once delivery with idempotency keys
- Data plane never blocked by control plane failures

### 2. **Plugin-Based Tool Registry**
- Tools self-register via Python entry points
- Add new integrations without touching core code
- Each tool: metadata, schema, cost tracking, rate limits

### 3. **Modular Agent Framework**
- Agents composed of tools (GraphAgent, MetricsAgent, etc.)
- Dynamic loading based on configuration
- Parallel execution where possible
- Unified result schema for easy synthesis

### 4. **Cheap Rules â†’ Smart LLM**
- Rule engine handles 60-80% of cases deterministically
- LLM planning only for ambiguous scenarios
- Massive cost savings + more predictable behavior

### 5. **Multi-Tenant Security**
- Per-tenant Kafka topics: `k8s-events-{tenant_id}`
- Client ID isolation in Neo4j, OpenSearch
- Per-tenant API credentials and budgets
- Secret management via Vault/AWS Parameter Store

### 6. **Observability & Cost Control**
- Every tool call tracked: latency, cost, success rate
- Per-tenant budgets and circuit breakers
- Audit trail for all agent decisions
- Prometheus metrics for monitoring

---

## ðŸ“ Project Structure

```
ai-sre-platform/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ ARCHITECTURE.md                    # Detailed architecture
â”œâ”€â”€ IMPLEMENTATION_ROADMAP.md          # Phase-by-phase guide
â”œâ”€â”€ CANONICAL_EVENT_ENVELOPE.md        # Event schema standard
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ TOOL_DEVELOPMENT_GUIDE.md     # How to add new tools
â”‚   â”œâ”€â”€ AGENT_DEVELOPMENT_GUIDE.md    # How to add new agents
â”‚   â”œâ”€â”€ KAFKA_INTEGRATION.md          # Kafka patterns
â”‚   â”œâ”€â”€ SECURITY.md                   # Security best practices
â”‚   â””â”€â”€ RUNBOOK.md                    # Operations guide
â”‚
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ diagrams/                     # Architecture diagrams
â”‚   â”œâ”€â”€ adr/                          # Architecture Decision Records
â”‚   â””â”€â”€ patterns/                     # Common patterns
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ event_envelope.avro       # Canonical envelope
â”‚   â”‚   â””â”€â”€ agent_result.json         # Agent result schema
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ tools.yaml                # Tool configurations
â”‚   â”‚   â””â”€â”€ credentials.yaml.example  # Credential template
â”‚   â””â”€â”€ agents/
â”‚       â””â”€â”€ agents.yaml               # Agent configurations
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tool_registry.py          # Tool plugin system
â”‚   â”‚   â”œâ”€â”€ agent_registry.py         # Agent management
â”‚   â”‚   â”œâ”€â”€ memory_manager.py         # Incident memory
â”‚   â”‚   â”œâ”€â”€ router.py                 # Rule-based routing
â”‚   â”‚   â””â”€â”€ schemas.py                # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ orchestrator.py           # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ planner.py                # LLM-based planning
â”‚   â”‚   â”œâ”€â”€ synthesizer.py            # Result synthesis
â”‚   â”‚   â””â”€â”€ budgets.py                # Cost/rate limiting
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                   # Base agent class
â”‚   â”‚   â”œâ”€â”€ graph_agent.py            # Neo4j causality
â”‚   â”‚   â”œâ”€â”€ metrics_agent.py          # Datadog/Prometheus
â”‚   â”‚   â”œâ”€â”€ logs_agent.py             # Argo/OpenSearch
â”‚   â”‚   â”œâ”€â”€ code_agent.py             # GitHub integration
â”‚   â”‚   â””â”€â”€ context_agent.py          # Slack/docs search
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                   # Base tool class
â”‚   â”‚   â”œâ”€â”€ graph_tools.py            # Neo4j tools
â”‚   â”‚   â”œâ”€â”€ metrics_tools.py          # Datadog/Prom tools
â”‚   â”‚   â”œâ”€â”€ logs_tools.py             # Log search tools
â”‚   â”‚   â”œâ”€â”€ code_tools.py             # GitHub tools
â”‚   â”‚   â””â”€â”€ context_tools.py          # Slack/doc tools
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kafka_consumer.py         # Multi-tenant consumer
â”‚   â”‚   â”œâ”€â”€ processors/               # Per-source processors
â”‚   â”‚   â”‚   â”œâ”€â”€ k8s_processor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ datadog_processor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ argo_processor.py
â”‚   â”‚   â”‚   â””â”€â”€ cicd_processor.py
â”‚   â”‚   â””â”€â”€ envelope.py               # Envelope validation
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                   # FastAPI app
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ investigate.py        # POST /investigate
â”‚   â”‚   â”‚   â”œâ”€â”€ incidents.py          # Incident CRUD
â”‚   â”‚   â”‚   â””â”€â”€ health.py             # Health checks
â”‚   â”‚   â””â”€â”€ middleware.py             # Auth, logging
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ secrets.py                # Secret management
â”‚       â”œâ”€â”€ circuit_breaker.py        # Circuit breakers
â”‚       â””â”€â”€ observability.py          # Metrics/tracing
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_kafka_topics.sh         # Create topics
â”‚   â”œâ”€â”€ load_test_data.py             # Load sample data
â”‚   â””â”€â”€ deploy.sh                     # Deployment script
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml            # Local dev stack
â”‚   â””â”€â”€ docker-compose.prod.yml       # Production stack
â”‚
â”œâ”€â”€ helm/                             # Kubernetes deployment
â”‚   â””â”€â”€ ai-sre-platform/
â”‚
â”œâ”€â”€ pyproject.toml                    # Python deps + entry points
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example                      # Environment variables
```

---

## ðŸš€ Quick Start

### Prerequisites

```bash
# Required services
- Kafka cluster (or local docker-compose)
- Neo4j 5.x (with vector index support)
- OpenSearch/Elasticsearch
- Redis
- PostgreSQL (optional, for service mapping)

# Python 3.11+
- pip install -r requirements.txt
```

### Local Development Setup

```bash
# 1. Clone and enter directory
cd mini-server-prod/kgroot-services/ai-sre-platform

# 2. Start infrastructure (Kafka, Neo4j, OpenSearch, Redis)
docker-compose up -d

# 3. Create Kafka topics
./scripts/setup_kafka_topics.sh

# 4. Set environment variables
cp .env.example .env
# Edit .env with your credentials

# 5. Initialize Neo4j indexes (from existing scripts)
# Run your existing CREATE-INDEXES.cypher

# 6. Start API server
uvicorn src.api.main:app --reload --port 8000

# 7. Start Kafka consumers (in separate terminals)
python -m src.ingestion.kafka_consumer --group k8s-events
python -m src.ingestion.kafka_consumer --group datadog-metrics

# 8. Test the system
curl -X POST http://localhost:8000/api/v1/investigate \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Why did nginx pods fail?",
    "tenant_id": "acme-01",
    "time_window": {
      "start": "2025-10-30T07:00:00Z",
      "end": "2025-10-30T08:00:00Z"
    }
  }'
```

---

## ðŸ“‹ Implementation Phases

> **See [IMPLEMENTATION_ROADMAP.md](./IMPLEMENTATION_ROADMAP.md) for detailed steps**

### Phase 1: Foundation (Week 1-2)
- âœ… Canonical event envelope
- âœ… Tool registry framework
- âœ… Agent base classes
- âœ… Basic orchestrator
- âœ… GraphAgent (wrap existing Neo4j RCA)

### Phase 2: Ingestion Layer (Week 3-4)
- âœ… Kafka topic setup
- âœ… Multi-tenant consumers
- âœ… Event processors (K8s, Datadog, Argo)
- âœ… Schema validation
- âœ… Dead-letter queues

### Phase 3: Core Agents (Week 5-8)
- âœ… MetricsAgent (Datadog/Prometheus)
- âœ… LogsAgent (Argo/OpenSearch)
- âœ… CodeAgent (GitHub integration)
- âœ… ContextAgent (Slack/docs)

### Phase 4: Intelligence Layer (Week 9-10)
- âœ… Rule-based router
- âœ… LLM planner (GPT-4o)
- âœ… Result synthesizer
- âœ… Incident memory & learning

### Phase 5: Production Hardening (Week 11-12)
- âœ… Circuit breakers
- âœ… Per-tenant budgets
- âœ… Observability (Prometheus, Grafana)
- âœ… Security (Vault integration)
- âœ… Load testing

### Phase 6: Advanced Features (Week 13+)
- âœ… Vector similarity for past incidents
- âœ… Automated runbook generation
- âœ… Proactive anomaly detection
- âœ… Human-in-the-loop approvals

---

## ðŸ”‘ Key Concepts

### Canonical Event Envelope

Every event from any source is normalized to:

```json
{
  "tenant_id": "acme-01",
  "source": "k8s.events",
  "event_type": "PodOOMKilled",
  "ts": "2025-10-30T07:41:03Z",
  "correlation_ids": {
    "trace_id": "abc123",
    "commit": "def456",
    "workflow": "argo-deploy-789"
  },
  "entity": {
    "cluster": "prod-us-west",
    "namespace": "payments",
    "service": "nginx-api",
    "pod": "nginx-api-7d8f-xyz"
  },
  "severity": "error",
  "payload": { "...raw vendor fields..." },
  "ingest_id": "uuid-for-deduplication",
  "schema_version": "v1"
}
```

### Incident Scope

Every investigation operates on a well-defined scope:

```json
{
  "tenant_id": "acme-01",
  "service": "nginx-api",
  "time_window": {
    "start": "2025-10-30T07:00:00Z",
    "end": "2025-10-30T08:00:00Z"
  },
  "pivot_event_id": "evt-123",
  "correlation_ids": ["trace-abc", "commit-def"]
}
```

### Unified Agent Result

Every agent returns:

```json
{
  "agent": "MetricsAgent",
  "findings": [
    {
      "kind": "anomaly.cpu",
      "detail": { "service": "nginx-api", "spike_factor": 3.2 },
      "confidence": 0.82
    }
  ],
  "artifacts": [
    { "type": "timeseries", "ref": "es://metrics/cpu-spike-123" }
  ],
  "cost": {
    "llm_usd": 0.002,
    "api_usd": 0.001
  },
  "latency_ms": 640
}
```

---

## ðŸ› ï¸ Tool Development

Adding a new tool is simple:

```python
# src/tools/my_new_tool.py
from src.core.tool_registry import BaseTool, ToolMetadata, tool_registry

class MyNewTool(BaseTool):
    def __init__(self, api_client):
        self.client = api_client
        tool_registry.register(self)  # Auto-register

    @property
    def metadata(self) -> ToolMetadata:
        return ToolMetadata(
            name="my_new_tool",
            description="Does something useful",
            category="metrics",
            rate_limit_per_min=60
        )

    async def execute(self, params: dict) -> dict:
        # Your logic here
        result = await self.client.query(params)
        return {"success": True, "data": result}

    def get_schema(self) -> dict:
        # JSON schema for LLM function calling
        return {...}
```

Enable in config:

```yaml
# config/tools/tools.yaml
tools:
  my_new_tool:
    enabled: true
    api_key: ${MY_TOOL_API_KEY}
```

**That's it!** The tool is now available to all agents.

---

## ðŸ“Š Observability

### Metrics (Prometheus)

```
# Tool calls
kgroot_tool_calls_total{tenant, tool, status}
kgroot_tool_latency_seconds{tenant, tool}
kgroot_tool_cost_usd{tenant, tool}

# Agent execution
kgroot_agent_executions_total{tenant, agent, status}
kgroot_agent_latency_seconds{tenant, agent}

# Orchestrator
kgroot_investigations_total{tenant, status}
kgroot_investigation_confidence{tenant}
```

### Tracing

OpenTelemetry traces for every investigation:
- Span per agent execution
- Span per tool call
- Span per LLM invocation

### Audit Trail

Every decision logged to `audit-orchestrator` Kafka topic:
```json
{
  "timestamp": "2025-10-30T07:45:00Z",
  "tenant_id": "acme-01",
  "incident_id": "inc-123",
  "event_type": "plan_created",
  "details": {
    "agents_selected": ["GraphAgent", "MetricsAgent"],
    "reasoning": "OOMKilled detected, need causality + metrics"
  }
}
```

---

## ðŸ”’ Security

- **Secrets**: Vault/AWS Parameter Store (never in code/config)
- **Multi-tenancy**: Strict isolation via tenant_id in all queries
- **API Auth**: JWT tokens with tenant claims
- **Secret Redaction**: Automatic PII/secret scrubbing in logs
- **Rate Limiting**: Per-tenant and per-tool rate limits
- **Circuit Breakers**: Prevent cascade failures

---

## ðŸ’° Cost Control

### Per-Tenant Budgets

```yaml
budgets:
  acme-01:
    max_llm_usd_per_day: 50.0
    max_agent_concurrency: 5
    max_investigations_per_hour: 100
```

### Tool-Level Tracking

Every tool call tracked:
- API cost (Datadog, OpenAI, etc.)
- Compute cost (estimate)
- Total per incident
- Daily/monthly rollups

---

## ðŸ§ª Testing Strategy

```bash
# Unit tests
pytest tests/unit/

# Integration tests (requires docker-compose)
pytest tests/integration/

# End-to-end tests
pytest tests/e2e/

# Load testing
locust -f tests/load/locustfile.py
```

---

## ðŸ“š Documentation

- [ARCHITECTURE.md](./ARCHITECTURE.md) - Deep dive into architecture
- [IMPLEMENTATION_ROADMAP.md](./IMPLEMENTATION_ROADMAP.md) - Step-by-step guide
- [CANONICAL_EVENT_ENVELOPE.md](./CANONICAL_EVENT_ENVELOPE.md) - Event schema
- [docs/TOOL_DEVELOPMENT_GUIDE.md](./docs/TOOL_DEVELOPMENT_GUIDE.md) - Adding tools
- [docs/AGENT_DEVELOPMENT_GUIDE.md](./docs/AGENT_DEVELOPMENT_GUIDE.md) - Adding agents
- [docs/KAFKA_INTEGRATION.md](./docs/KAFKA_INTEGRATION.md) - Kafka patterns
- [docs/RUNBOOK.md](./docs/RUNBOOK.md) - Operations guide

---

## ðŸŽ¯ Success Metrics

### Technical Metrics
- **MTTR Reduction**: 40-60% faster incident resolution
- **False Positive Rate**: <10% (vs 60% in naive approaches)
- **Confidence Score**: >0.85 for root causes
- **Agent Latency**: p95 < 5 seconds per agent
- **Cost per Investigation**: <$0.50

### Business Metrics
- **On-call Load**: 30-50% reduction in pages
- **Toil Reduction**: 70% of investigations automated
- **Knowledge Capture**: 100% of incidents stored for learning
- **Onboarding Time**: 6 months â†’ 2 weeks (via AI SRE training)

---

## ðŸ¤ Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md) for:
- Code style guide
- PR process
- Testing requirements
- ADR process

---

## ðŸ“„ License

Internal use only - Proprietary

---

## ðŸš¦ Current Status

**Phase**: Design & Architecture âœ…
**Next**: Phase 1 Implementation (Foundation)

---

## ðŸ“ž Support

- Slack: `#ai-sre-platform`
- Email: `sre-team@company.com`
- Docs: `https://docs.company.com/ai-sre`

---

**Let's build a true AI SRE! ðŸš€**